#garak works with language models and any tech using them to determine where the security holes can be with each solution. 

#Install on Mac:

virtualenv garak
cd garak
source bin/activate 

pin install setuptools
pip install Cmake
git clone https://github.com/google/sentencepiece.git 
cd sentencepiece
mkdir build
cd build
cmake .. -DSPM_ENABLE_SHARED=OFF -DCMAKE_INSTALL_PREFIX=./root
make install
cd ../python
python setup.py bdist_wheel
pip install dist/sentencepiece*.whl
cd ../..

python -m pip install -U garak

#test run:
python -m garak --model_type test.Blank --probes test.Test


#Using Garak:

python3 -m garak --list_probes		-> shows test cases


Create file garak-config.json:
{"rest.RestGenerator":
    {
        "name": "example service",
        "uri": "https://example.ai/llm",
        "method": "post",
        "headers":{
            "X-Authorization": "$KEY",
	    "Content-Type": "application/json"
        },
        "req_template_json_object":{
            "text":"$INPUT"
        },
        "response_json": true,
        "response_json_field": "text"
    }
}

# $INPUT is the location that grok will inject it's prompts/attacks
# Another example of this might look like this:
# "req_template_json_object":{
#     "api_provider":"groq",
#     "chat_history":
#         [
#             {"role":"user","content":"$INPUT"}
#         ]

# The response_json_field "text" is the parameter that contains the response that 
# will be evaluated to see if the attack worked

# $KEY value is acquired from an environment variable on the system.
# export REST_API_KEY=<authorization_key>

python3 -m garak --model_type huggingface --model_name gpt2 -G ./garak-config.json --probes lmrc.Profanity

python3 -m garak --model_type huggingface --model_name ibm-granite/granite-3.1-1b-a400m-instruct  -G ./garak-config.json --probes lmrc.Profanity

python3 -m garak --model_type openai --model_name gpt-3.5-turbo -G ./garak-config.json --probes xss.MarkdownImageExfil


Proxy - add to garak-config.json file?:
{
    "rest": {
        "RestGenerator": {
           "uri": "http://my_internal_llm_endpoint",
           "proxies": {
              "http": "http://localhost:8080",
              "https": "https://localhost:8443",
           }
        }
    }
}
